# Distributed-ML-with-Apache-Spark
A scalable Big Data pipeline built with Apache Spark for preprocessing, feature engineering, and supervised machine learning to classify financial transactions. Includes distributed data processing, model evaluation, and performance optimization.


The project was completed as part of **Modern Data Science (SIT742)** coursework.

---

##  Objectives
- Process large-scale transactional data using Spark's distributed architecture  
- Perform feature engineering and data cleaning at scale  
- Train supervised ML models such as:
  - Logistic Regression  
  - Random Forest  
  - Gradient-Boosted Trees  
- Evaluate model accuracy and scalability  
- Demonstrate Spark MLlib capabilities for real-world classification tasks

---

##  Technologies & Tools
- **Apache Spark**
- **Spark MLlib**
- **Python (PySpark)**
- **Big Data Processing Pipelines**
- **Jupyter Notebook / Databricks-like workflow**

